{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6aaac7b3",
      "metadata": {
        "id": "6aaac7b3"
      },
      "source": [
        "# Exploring Retrieval-Augmented Generation (RAG) vs Plain Chat Completions with HR Data\n",
        "\n",
        "*This notebook demonstrates how a simple LLM chat compares to a RAG-powered approach when answering company-specific HR questions.*\n",
        "\n",
        "**Get your API key:**\n",
        "[Google AI Studio](https://makersuite.google.com/app/apikey) or\n",
        "[Openai](https://platform.openai.com/settings/organization/api-keys)\n",
        " ‚Üí Create API Key ‚Üí Copy to config below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67972bce",
      "metadata": {
        "id": "67972bce"
      },
      "source": [
        "## üì¶ Install Packages\n",
        "\n",
        "**Run this cell to install dependencies:**\n",
        "<p> Ignore Google colab package install errors. They are specific to only the colab environment</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede0847b",
      "metadata": {
        "id": "ede0847b"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  !pip install -q langchain langchain-google-genai langchain-community langchain-openai langchain-chroma\n",
        "  !pip install -q python-dotenv\n",
        "  !pip install -q matplotlib\n",
        "  !pip install -q plotly\n",
        "  !pip install -q scikit-learn\n",
        "  !pip install -q requests\n",
        "  !pip install -q gradio\n",
        "\n",
        "  print(\"‚úÖ Packages installed!\")\n",
        "except Exception as e:\n",
        "  print(f\"‚ùå Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4912bb5",
      "metadata": {
        "id": "b4912bb5"
      },
      "source": [
        "## üìö Imports\n",
        "\n",
        "**Loading libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3789c0ee",
      "metadata": {
        "id": "3789c0ee"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import warnings\n",
        "import os\n",
        "#env imports\n",
        "from google.colab import userdata\n",
        "#data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import gradio\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"‚úÖ Libraries loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bff99b9",
      "metadata": {
        "id": "7bff99b9"
      },
      "source": [
        "## üîß Configuration\n",
        "\n",
        "**Set your API key and preferences:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d689d6",
      "metadata": {
        "id": "16d689d6"
      },
      "outputs": [],
      "source": [
        "# API Configuration\n",
        "# uncomment the preferered provider\n",
        "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Model Configuration\n",
        "# MODEL_NAME = \"gemini-2.0-flash\"\n",
        "MODEL_NAME = \"gpt-4o-mini\" # for avaialable models visit https://platform.openai.com/docs/models\n",
        "\n",
        "# Chat Settings\n",
        "SYSTEM_PROMPT = \"\"\"You are an AI assistant trained to help with workplace and HR-related questions.\n",
        " Provide professional, well-structured answers using your general understanding of HR practices, company culture, and employee roles.\"\"\"\n",
        "# GOOGLE_CONFIG = {\n",
        "#     \"temperature\": 0.7,       # creativity vs determinism\n",
        "#     \"max_output_tokens\": 200, # length of response\n",
        "# }\n",
        "# google llm\n",
        "# llm = ChatGoogleGenerativeAI(\n",
        "#     google_api_key=GOOGLE_API_KEY,\n",
        "#     model=MODEL_NAME,\n",
        "#     **GOOGLE_CONFIG\n",
        "# )\n",
        "\n",
        "OPENAI_CONFIG = {\n",
        "    \"temperature\": 0.7,  # creativity vs determinism\n",
        "    \"max_tokens\": 200, # length of response\n",
        "}\n",
        "#Open AI llm\n",
        "llm = ChatOpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    model_name=MODEL_NAME,\n",
        "    **OPENAI_CONFIG\n",
        ")\n",
        "\n",
        "# RAG Settings\n",
        "CHUNK_SIZE = 200\n",
        "CHUNK_OVERLAP = 20\n",
        "TOP_K_RESULTS = 2\n",
        "\n",
        "# Vector Store (ChromaDB)\n",
        "COLLECTION_NAME = \"hr_handbook\"\n",
        "\n",
        "print(f\"‚úÖ Config ready | Model: {MODEL_NAME} | Vector Store: ChromaDB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0caa3e",
      "metadata": {
        "id": "6f0caa3e"
      },
      "source": [
        "\n",
        "\n",
        "## üí¨ Basic Chat\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d51baf",
      "metadata": {
        "id": "21d51baf"
      },
      "outputs": [],
      "source": [
        "# Initialize Gemini model\n",
        "\n",
        "def simple_chat(user_prompt: str) -> str:\n",
        "    \"\"\"Simple chat with Gemini\"\"\"\n",
        "    try:\n",
        "      messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "      ]\n",
        "\n",
        "      response = llm.invoke(messages)\n",
        "      return response.content\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "print(\"‚úÖ Chat model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd23e991",
      "metadata": {
        "id": "bd23e991"
      },
      "source": [
        "## üß™ Test Chat\n",
        "\n",
        "**Try the basic chat:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a5905e",
      "metadata": {
        "id": "b3a5905e"
      },
      "outputs": [],
      "source": [
        "# Test basic chat\n",
        "response = simple_chat(\"What is the leave policy of our company?\")\n",
        "print(\"ü§ñ Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f7745f5",
      "metadata": {
        "id": "8f7745f5"
      },
      "source": [
        "## üóÑÔ∏è Initialize Embeddings\n",
        "\n",
        "**Setup Google embeddings model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da1855e",
      "metadata": {
        "id": "8da1855e"
      },
      "outputs": [],
      "source": [
        "# Initialize Google embeddings\n",
        "# embeddings = GoogleGenerativeAIEmbeddings(\n",
        "#     model = 'models/gemini-embedding-001',\n",
        "#     google_api_key=GOOGLE_API_KEY\n",
        "# )\n",
        "\n",
        "# Initialize Openai embeddings\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Embeddings model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1eb70c",
      "metadata": {
        "id": "ab1eb70c"
      },
      "source": [
        "## üìÑ Process Documents\n",
        "\n",
        "**Load and chunk the sample documents:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a726a968",
      "metadata": {
        "id": "a726a968"
      },
      "outputs": [],
      "source": [
        "# Setup text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Load and process documents from the 'hr_data' folder\n",
        "documents = []\n",
        "hr_handbook_path = \"hr_data/hr_handbook.txt\" # Assuming it's a text file\n",
        "if os.path.exists(hr_handbook_path):\n",
        "    loader = TextLoader(hr_handbook_path)\n",
        "    docs = loader.load()\n",
        "    documents.extend(text_splitter.split_documents(docs))\n",
        "else:\n",
        "    print(f\"‚ùå Error: File not found at {hr_handbook_path}\")\n",
        "\n",
        "\n",
        "print(f\"üìÑ Processed {len(documents)} document chunks from {hr_handbook_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a66ec5",
      "metadata": {
        "id": "68a66ec5"
      },
      "source": [
        "## üóÑÔ∏è Create ChromaDB\n",
        "\n",
        "**Build the vector database:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3674559b",
      "metadata": {
        "id": "3674559b"
      },
      "outputs": [],
      "source": [
        "# Create ChromaDB vector store\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ ChromaDB created with {len(documents)} chunks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bSs354nGgge5",
      "metadata": {
        "id": "bSs354nGgge5"
      },
      "source": [
        "#\n",
        "## üïπÔ∏è Visualizing the Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uamTN0OuhIvI",
      "metadata": {
        "id": "uamTN0OuhIvI"
      },
      "outputs": [],
      "source": [
        "collection = vectorstore._collection\n",
        "result = collection.get(include=['embeddings', 'documents'])\n",
        "vectors = np.array(result['embeddings'])\n",
        "documents = result['documents']\n",
        "\n",
        "tsne = TSNE(n_components=3, random_state=42, perplexity=5)\n",
        "reduced_vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "# Create the 3D scatter plot\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=reduced_vectors[:, 0],\n",
        "    y=reduced_vectors[:, 1],\n",
        "    z=reduced_vectors[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(size=5, color='blue', opacity=0.8),\n",
        "    text=[f\"Text: {d[:100]}...\" for d in documents],\n",
        "    hoverinfo='text'\n",
        ")])\n",
        "\n",
        "fig.update_layout(\n",
        "    title='3D Chroma Vector Store Visualization',\n",
        "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
        "    width=900,\n",
        "    height=700,\n",
        "    margin=dict(r=20, b=10, l=10, t=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b12b4b01",
      "metadata": {
        "id": "b12b4b01"
      },
      "source": [
        "## üîç Setup Retriever\n",
        "\n",
        "**Configure document retrieval:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e657d53",
      "metadata": {
        "id": "2e657d53"
      },
      "outputs": [],
      "source": [
        "# Setup retriever from vectorstore\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K_RESULTS})\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"Who is Grace Williams\"\n",
        "retrieved_docs = retriever.get_relevant_documents(test_query)\n",
        "\n",
        "print(f\"‚úÖ Retriever ready! Test retrieved {len(retrieved_docs)} documents for: '{test_query}'\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "\n",
        "    filename = doc.metadata.get('source', 'Unknown').split('/')[-1]\n",
        "    print(f\"  {i}. {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ad968f",
      "metadata": {
        "id": "81ad968f"
      },
      "source": [
        "## üîó Setup Memory\n",
        "\n",
        "**Configure Langchain memory for llm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba6ce87",
      "metadata": {
        "id": "fba6ce87"
      },
      "outputs": [],
      "source": [
        "# Setup memory for conversational RAG\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "# Create RAG chain WITH memory (conversational)\n",
        "rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    #verbose=True #print context sent to llm\n",
        ")\n",
        "\n",
        "\n",
        "def rag_chat(question: str, chat_history):\n",
        "    \"\"\"RAG with conversation memory\"\"\"\n",
        "    try:\n",
        "        result = rag_chain({\"question\": question, \"chat_history\": chat_history})\n",
        "        return result[\"answer\"]\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\", []\n",
        "\n",
        "print(\"‚úÖ RAG setup ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-gptyNX-QlNU",
      "metadata": {
        "id": "-gptyNX-QlNU"
      },
      "source": [
        "# üß™ Test RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W9mYZOcuQv27",
      "metadata": {
        "id": "W9mYZOcuQv27"
      },
      "outputs": [],
      "source": [
        "question = \"What is the leave policy of our company?\"\n",
        "rag_response = rag_chat(question, chat_history=[])\n",
        "print(\"ü§ñ RAG RESPONSE:\")\n",
        "print(rag_response)\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c47d2b54",
      "metadata": {
        "id": "c47d2b54"
      },
      "source": [
        "## ‚öñÔ∏è Compare: Chat vs RAG\n",
        "\n",
        "**Test the same question with both approaches:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a912105",
      "metadata": {
        "id": "7a912105"
      },
      "outputs": [],
      "source": [
        "# Test question about specific information in our documents\n",
        "test_question = \"Can I work from home?\"\n",
        "\n",
        "print(\"üîç TEST QUESTION:\")\n",
        "print(test_question)\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6e9dfe",
      "metadata": {
        "id": "4f6e9dfe"
      },
      "source": [
        "## üí¨ Step 1: Regular Chat\n",
        "\n",
        "**Ask without document context:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de02ce26",
      "metadata": {
        "id": "de02ce26"
      },
      "outputs": [],
      "source": [
        "# Regular chat (no RAG)\n",
        "regular_response = simple_chat(test_question)\n",
        "\n",
        "print(\"üí¨ REGULAR CHAT RESPONSE:\")\n",
        "print(regular_response)\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b14865e",
      "metadata": {
        "id": "7b14865e"
      },
      "source": [
        "## üîç Step 2: RAG Chat\n",
        "\n",
        "**Ask with document context:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52aafadc",
      "metadata": {
        "id": "52aafadc"
      },
      "outputs": [],
      "source": [
        "# RAG chat memory\n",
        "rag_response = rag_chat(test_question)\n",
        "\n",
        "print(\"üîç RAG CHAT RESPONSE:\")\n",
        "print(rag_response)\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
